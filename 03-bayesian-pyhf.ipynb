{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade pip\n",
    "# ! pip install --user numpy scipy matplotlib pyhf iminuit json corner\n",
    "# ! pip install git+https://github.com/malin-horstmann/bayesian_pyhf.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyhf\n",
    "import matplotlib.pyplot as plt\n",
    "from bayesian_pyhf import infer, prepare_inference, plotting\n",
    "import pymc as pm\n",
    "import corner\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian analysis with `pyhf`\n",
    "\n",
    "The main job of `pyhf` is to supply a HistFactory likelihood template,\n",
    "\n",
    "$$\n",
    "        L(\\boldsymbol{n}, \\boldsymbol{a} \\mid \\boldsymbol{\\eta}, \\boldsymbol{\\chi})=\\underbrace{\\operatorname{Pois}\\left(\n",
    "        \\boldsymbol{n} \\mid\n",
    "        \\boldsymbol{\\nu}(\\boldsymbol{\\eta}, \\boldsymbol{\\chi})\n",
    "        \\right)}_{\\text{data likelihood}}\n",
    "        \\underbrace{c\\left(\\boldsymbol{a} \\mid \\boldsymbol{\\chi} \\right)}_{\\text{constraint likelihood}}.\n",
    "$$\n",
    "\n",
    "In addition, we have seen other features, which allow for its minimization, etc.\n",
    "\n",
    "The likelihood is a very general object, used for both frequentist and Bayesian analyses.\n",
    "The constraint likelihood is the frequentist way of injecting prior knowledge for certain parameters into the full likelihood.\n",
    "\n",
    "Due to limited computational power in the past, HEP has widely commited to frequentist methods, but situations exist, where the commonly used tools are not the most handy. For example, dealing with multiple parameters of interest leads to non-trivial behaviour and the standard asymptotic formulae do not apply. Physically bounded parameters also introduce issues for expected asymptotic behaviours.\n",
    "\n",
    "Bayesian methods have a slightly different approach to statistical inference. For example, a more handy way of dealing with multiple parameters of interest and less reliance on asymptotics for more complicated inference. The main object of a Bayesian analysis is the posterior distribution for a set of parameters $\\boldsymbol{\\theta}$, given some data $\\boldsymbol{x}$,\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\theta} \\mid \\boldsymbol{x}) \\propto p(\\boldsymbol{x} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}),\n",
    "$$\n",
    "\n",
    "where $p(\\boldsymbol{x} \\mid \\boldsymbol{\\theta})$ is the likelihood and $p(\\boldsymbol{\\theta})$ are priors for our parameters. Any further statistical quantity can be derived from the posterior.\n",
    "\n",
    "Using `pyhf` HistFactory likelihoods to obtain a posterior is as simple as translating the constriant likelihood into priors and introducing additional priors for the unconstrained parameters,\n",
    "\n",
    "$$\n",
    "    p\\left( \\boldsymbol{\\eta}, \\boldsymbol{\\chi} \\vert \\boldsymbol{n}, \\boldsymbol{a} \\right) \\propto\n",
    "    \\underbrace{\\operatorname{Pois}\\left(\n",
    "    \\boldsymbol{n} \\mid\n",
    "    \\boldsymbol{\\nu}(\\boldsymbol{\\eta}, \\boldsymbol{\\chi})\n",
    "    \\right)}_{\\text{data likelihood}}\n",
    "    \\quad\n",
    "    \\underbrace{p\\left( \\boldsymbol{\\chi} | \\boldsymbol{a} \\right)}_{\\text{constraint prior}}\n",
    "    \\quad\n",
    "    \\underbrace{p\\left( \\boldsymbol{\\eta} \\right)}_{\\text{unconstraint prior}}.\n",
    "$$\n",
    "\n",
    "<span style=\"color:red\">\n",
    "How would you construct the constraint prior?\n",
    "</span>\n",
    "\n",
    "A tool for this translation exists: [bayesian_pyhf](https://github.com/malin-horstmann/bayesian_pyhf) which is explained in detail [here](https://arxiv.org/pdf/2309.17005). We will explore it here on a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pyhf.simplemodels.correlated_background(\n",
    "            signal=[50, 100], bkg=[500, 600], bkg_down=[490, 580], bkg_up=[510, 620]\n",
    "        )\n",
    "\n",
    "data = [600, 800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = pyhf.simplemodels.uncorrelated_background(\n",
    "#             signal=[50, 100], bkg=[500, 600], bkg_uncertainty=[20, 20]\n",
    "#         )\n",
    "\n",
    "# data = [600, 800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fit `pyhf` model\n",
    "\n",
    "Before evaluating the next cell: What result do you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhf.set_backend(\"jax\", pyhf.optimize.minuit_optimizer())\n",
    "best_fit = pyhf.infer.mle.fit(data+model.config.auxdata, model, return_uncertainties=True)\n",
    "best_fit.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayesian inference\n",
    "\n",
    "One of the most controversial choices in a Bayesian analysis is the choice of a good prior. There are as many opinions on priors as Bayesian analysts (see eg. [here](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)).\n",
    "\n",
    "<span style=\"color:red\">\n",
    "What prior would you chose for our signal strength?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconstr_priors = {\n",
    "    'mu': {'type': 'Normal', 'mu': [1.], 'sigma': [5.]},\n",
    "    # 'mu': {'type': 'Uniform_Unconstrained', 'lower': [1.], 'upper': [3.]},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cool thing is that the auxiliary data, used to constrain parameters in the HistFactory likelihood can be used to build the constraint prior\n",
    "$$\n",
    "p(\\chi \\mid a) \\propto p(a \\mid \\chi) p(\\chi).\n",
    "$$\n",
    "\n",
    "For one Gaussian distributed auxiliary measurement,\n",
    "$$\n",
    "p(a \\mid \\chi) = \\mathcal{N}(a, \\sigma_{\\mathrm{aux}}), \\qquad\n",
    "p(\\chi) = \\mathcal{N}(\\mu_0, \\sigma_0).\n",
    "$$\n",
    "Where $\\mu_0, \\sigma_0$ are free choices for the ur-prior.\n",
    "\n",
    "The constraint prior can then be derived analytically as\n",
    "$$\n",
    "p(\\chi \\mid a) = \\mathcal{N}(\\mu', \\sigma'),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mu^{\\prime}=\\frac{\\sigma_{\\mathrm{aux}}^2 \\sigma_0^2}{\\sigma_{\\mathrm{aux}}^2+\\sigma_0^2}\\left(\\frac{\\mu_0}{\\sigma_0^2}+\\frac{a}{\\sigma_{\\mathrm{aux}}^2}\\right), \\quad \\sigma^{\\prime}=\\frac{\\sigma_{\\mathrm{aux}}^2 \\sigma_0^2}{\\sigma_{\\mathrm{aux}}^2+\\sigma_0^2}\n",
    "$$\n",
    "\n",
    "The freedom to choose these compatible ur-priors is justified as the priors will be dominated by the auxiliary measurements. Especially,\n",
    "$$\n",
    "\\sigma_{\\mathrm{ur}} \\gg \\sigma_{\\mathrm{aux}} \\quad \\longrightarrow \\quad \\mu^{\\prime} \\rightarrow a, \\quad \\sigma^{\\prime} \\rightarrow \\sigma_{\\mathrm{aux}}.\n",
    "$$\n",
    "\n",
    "For more details, and the equivalent derivation for the Poisson (Gamma) distribution, see [bayesian pyhf paper](https://arxiv.org/pdf/2309.17005) or the [original paper](https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf).\n",
    "\n",
    "Let us look at the priors of our simple model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priorDict = prepare_inference.build_priorDict(model, unconstr_priors)\n",
    "priorDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Check this for our background uncertainty:\n",
    "\n",
    "Hint: in `bayesian_pyhf` $\\mu_0= 0, \\sigma_0=2$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaults set in bayesian_pyhf\n",
    "mu_0 = 0\n",
    "sigma_0 = 2\n",
    "\n",
    "aux_obs = model.config.auxdata[0]\n",
    "sugma_aux = float(model.constraint_model.constraints_gaussian.sigmas[0])\n",
    "\n",
    "sigma_prime = ...\n",
    "mu_prime = ...\n",
    "\n",
    "mu_prime, sigma_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling process is by far the most advanced step in a Bayesian analysis. For a long time Bayesian analyses have been limited by computational power. Nowadays, a number of libraries exist that implement very advanced sampling algorithms ([see here for a good overview]). Here we use [pymc](https://www.pymc.io/welcome.html) which makes use of Hamiltonian MC sampling ([more details here](https://arxiv.org/pdf/1701.02434)).\n",
    "\n",
    "First we find the mode of the posterior, using a Maximum A Posteriori (MAP) estimate. This can be compared to the best fit point of a frequentist analysis.\n",
    "\n",
    "Then we perform a minimal sampling, with 10000 draws and 1000 tune in samples in 4 chains. The chains are automatically distributed across different cores (unless otherwise specified).\n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "How many samples are enough?\n",
    "\n",
    "Why are multiple chains important?\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with infer.model(model, unconstr_priors, data):\n",
    "    MAP = pm.find_MAP()\n",
    "    post_data = pm.sample(draws=10000, tune=1000, chains=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Does the mode of the posterior agree with the best fit point we found earlier?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the posterior\n",
    "\n",
    "A common way to visualize a posterior is through **corner plots**. Usually, one only plots the marginal posterior,\n",
    "$$\n",
    "p\\left( \\boldsymbol{\\eta} \\vert \\boldsymbol{x} \\right) = \\int d\\boldsymbol{\\chi} ~  \\ p\\left( \\boldsymbol{\\eta}, \\boldsymbol{\\chi} \\vert \\boldsymbol{x} \\right)\n",
    "$$\n",
    "where we integrate out all nuisance parameters and keep only parameters of interest. Here we only have two parameters, so we plot all.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "Where does our fest fit point lie on this plot?\n",
    "\n",
    "Are the parameters correlated? If so, so you understand the correlation?\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.corner(post_data.posterior);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing uncertainties\n",
    "\n",
    "The posterior is the central object of any Bayesian analysis. Conventions are slightly different compared to frequentist analyses. **Iff** the posterior distribution is Gaussian, it makes sense to quote results as $\\mu \\pm \\sigma$.\n",
    "\n",
    "Below is a summary of results derived from the posterior samples.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "Can you read off $\\mu$ and $\\sigma$, derived from the posterior?\n",
    "\n",
    "Is the mode of a marginal posterior the same as the peaks of the posterior? If not, why not?\n",
    "\n",
    "Does the distribution of $\\mu$ deviate from a Gaussian distribution?\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(post_data.posterior, hdi_prob=0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
