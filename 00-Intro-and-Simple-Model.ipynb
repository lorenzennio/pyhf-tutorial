{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhf\n",
    "import pyhf.contrib.utils\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# for plotting / tutorial purposes\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# for interactivity\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HistFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyhf` stands for **py**thon-based **H**ist**F**actory.\n",
    "\n",
    "It's a tool for statistical analysis of data in High Energy Physics.\n",
    "\n",
    "In this chapter, we will cover\n",
    "* What HistFactory is in general\n",
    "* What pyhf is specifically (and what it is not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis\n",
    "\n",
    "We divide analyses into the type of fit being performed:\n",
    "* unbinned analysis (based on individual observed events)\n",
    "* binned analyses (based on aggregation of events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img alt=\"WHgamgam unbinned distribution\" src=\"https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SUSY-2018-23/fig_04d.png\" width=300 style=\"display: inline\" />\n",
    "    <img alt=\"SUSY MBJ binned distribution\" src=\"https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/CONFNOTES/ATLAS-CONF-2018-041/fig_08a.png\" width=400 style=\"display: inline\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like HistFactory, `pyhf` does not work with unbinned analyses. These will not be covered in the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Binned?\n",
    "\n",
    "Most likely, one performs a binned analysis if no functional form of the p.d.f. is known. Instead, you make approximations (re: educated guesses) as to this functional form through histograms.\n",
    "\n",
    "What is a histogram? Fundamentally, a histogram is a tool to bookkeep arrays of numbers:\n",
    "* binning\n",
    "* counts\n",
    "* uncertainties\n",
    "\n",
    "Beyond that, it contains helpful ingredients to make them more user-friendly for common operations (addition, division, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the ingredients?\n",
    "\n",
    "Once you have a model, you can perform inference such as\n",
    "* exclusion fit (upper limits)\n",
    "* discovery fit (lower limits)\n",
    "* measurement (two-sided intervals)\n",
    "* parameter scans\n",
    "* impact plots\n",
    "* pull plots\n",
    "* ...\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/scikit-hep/pyhf/main/docs/_static/img/README_1bin_example.png\" alt=\"common operation - parameter scan\" width=400 />\n",
    "\n",
    "Let's make up some samples and histograms to go along with it to understand what's going on. Suppose we have an analysis with expected event rate $\\lambda$ and measurements $n$. For this simple case, the overall probability of the full experiment is the **joint probability** of each bin:\n",
    "\n",
    "$$\n",
    "p(n|\\lambda) = \\prod_{\\mathrm{bin}\\ b} \\mathrm{Pois}(n_b | \\lambda_b)\n",
    "$$\n",
    "\n",
    "A Poisson model is used as we are performing a counting experiment (counting the number of random events with an expected rate) in each bin of the observable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [1, 2, 3]\n",
    "observed = [3, 4, 4]\n",
    "expected_yields = [3.7, 3.2, 2.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(bins, expected_yields, 1.0, label=r\"Expected\", edgecolor=\"blue\", alpha=0.5)\n",
    "ax.scatter(bins, [3, 4, 4], color=\"black\", label=\"Observed\")\n",
    "ax.set_ylim(0, 6)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Observable\", fontsize=12)\n",
    "ax.set_ylabel(\"Count\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we don't always often have just a single expected (simulation) sample, and $\\lambda$ is often the sum of multiple sample yields\n",
    "\n",
    "$$\n",
    "\\lambda = \\sum_{\\mathrm{sample}\\ s} \\lambda_s\n",
    "$$\n",
    "\n",
    "A typical case might be multiple (sub)dominant backgrounds or having a model where the observed events are described by a signal + background p.d.f. The model is then\n",
    "\n",
    "$$\n",
    "p(n|\\lambda) = \\prod_{\\mathrm{bin}\\ b} \\mathrm{Pois}(n_b | \\lambda_b) \\qquad \\lambda_b = \\sum_{\\mathrm{sample}\\ s} \\lambda_{bs}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [1, 2, 3]\n",
    "observed = [3, 4, 4]\n",
    "background = [3.0, 1.5, 1.0]\n",
    "signal = [0.7, 1.7, 1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(bins, background, 1.0, label=r\"Background\", edgecolor=\"red\", alpha=0.5)\n",
    "ax.bar(\n",
    "    bins, signal, 1.0, label=r\"Signal\", edgecolor=\"blue\", bottom=background, alpha=0.5\n",
    ")\n",
    "ax.scatter(bins, [3, 4, 4], color=\"black\", label=\"Observed\")\n",
    "ax.set_ylim(0, 6)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Observable\", fontsize=12)\n",
    "ax.set_ylabel(\"Count\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already, you can see the p.d.f. for this simple case starts expanding to be a little bit more generic, and a little bit more flexible. Now we want to incorporate when the expected yields for signal and backgrounds depend on some **parameters**, perhaps how we applied calibrations to some objects, or how we configured our Monte-Carlo generators, etc.\n",
    "\n",
    "Suppose we wanted a a normalization factor $\\mu_s$ scaling up (or down!) the sample. For example, if we want to parametrize the signal strength (without changing background). So $\\lambda$ becomes a function of $\\theta = \\{\\mu\\}$ (a set of the parameters that determine the expected event rate), then our p.d.f. expands to be\n",
    "\n",
    "$$\n",
    "p(n|\\lambda(\\mu)) = \\prod_{\\mathrm{bin}\\ b} \\mathrm{Pois}(n_b | \\lambda_b(\\theta)) \\qquad \\lambda_b(\\theta) = \\sum_{\\mathrm{sample}\\ s} \\lambda_{bs}(\\theta)\n",
    "$$\n",
    "\n",
    "where $\\mu_{\\mathrm{background}} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(mu=(0, 5, 0.1))\n",
    "def draw_plot(mu=1):\n",
    "    bins = [1, 2, 3]\n",
    "    observed = [3, 4, 4]\n",
    "    background = [3.0, 1.5, 1.0]\n",
    "    signal = [i * mu for i in [0.7, 1.7, 1.5]]\n",
    "\n",
    "    print(f\"signal: {signal}\")\n",
    "    print(f\"background: {background}\")\n",
    "    print(f\"observed: {observed}\\n\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(bins, background, 1.0, label=r\"Background\", edgecolor=\"red\", alpha=0.5)\n",
    "    ax.bar(\n",
    "        bins,\n",
    "        signal,\n",
    "        1.0,\n",
    "        label=r\"Signal\",\n",
    "        edgecolor=\"blue\",\n",
    "        bottom=background,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.scatter(bins, [3, 4, 4], color=\"black\", label=\"Observed\")\n",
    "    ax.set_ylim(0, 6)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Observable\", fontsize=12)\n",
    "    ax.set_ylabel(\"Count\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final thing to finish our build up of a simplified HistFactory model is the concept of **auxiliary measurements**. Perhaps the background sample rate is modified by some normalization parameter, and we've made measurements of this parameter in a separate analysis (e.g. studies of the Jet Energy Scale). These prior experimental studies give a constraint that the parameter lies within a certain range.\n",
    "\n",
    "For some parameters in a statistical model we don't have prior experimental evidence for their values and must infer its values is the given analysis. These are **unconstrained** parameters ($\\eta$) and enter into the main model as parameters of the event rate $\\lambda(\\theta)$\n",
    "\n",
    "$$\n",
    "p(n | \\lambda(\\theta)).\n",
    "$$\n",
    "\n",
    "For many model parameters, their values in the model are constrained by a _constraint term function_, included in the model along with the the main model p.d.f, which describes **auxiliary measurements/data** ($a$) about the model parameter. These are **constrained** parameters ($\\chi$) and enter into the model both in the constraint terms and as parameters of the event rate $\\lambda(\\theta)$\n",
    "\n",
    "$$\n",
    "p_\\chi(a | \\chi)\n",
    "$$\n",
    "\n",
    "where $\\theta = \\{\\eta, \\chi\\}$. This constraining function model is chosen by the physics it represents, but in HistFactory most constraint terms are modeled as a Normal (Gaussian) or Poisson.\n",
    "\n",
    "With the constraint terms the model expands to be\n",
    "\n",
    "$$\n",
    "p(n,a|\\lambda(\\theta)) = \\prod_{\\mathrm{bin}\\ b} \\mathrm{Pois}(n_b | \\lambda_b(\\theta)) \\prod_{\\mathrm{constraint}\\ \\chi} p_\\chi(a_\\chi | \\chi) \\qquad \\lambda_b(\\theta) = \\sum_{\\mathrm{sample}\\ s} \\lambda_{bs}(\\theta)\n",
    "$$\n",
    "\n",
    "where the expected event rate $\\lambda_b(\\theta)$ is modified from its nominal value by a **chosen interpolation function** that smoothly interpolates between the up- and down-variations $(\\pm1 \\sigma)$ of the constraint term to provide an event rate modifier for any value of $\\chi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this simple example, let's consider a constraint term of a Normal distribution centered at $\\mu=0$ (\"auxiliary measurement\" $a=0$) with $\\sigma=1$ for constraining the normalization on the background where an up-variation ($\\mu_b = +1$) scales by 1.3, and a down-variation ($\\mu_b = -1$) scales by 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_constraint(mu_b=0.0):\n",
    "    # auxiliary measurement of 0\n",
    "    # though note that for Normal observation and mean are symmetric under exchange\n",
    "    return norm.pdf(0.0, loc=mu_b, scale=1.0)\n",
    "\n",
    "\n",
    "# selected interpolation function\n",
    "def interpolate(down, nom, up, alpha):\n",
    "    if alpha >= 0:\n",
    "        return (up - nom) * alpha + 1\n",
    "    else:\n",
    "        return 1 - (down - nom) * alpha\n",
    "\n",
    "\n",
    "@widgets.interact(mu=(0, 5, 0.1), mu_b=(-1, 1, 0.1))\n",
    "def draw_plot(mu=1, mu_b=0):\n",
    "    bins = [1, 2, 3]\n",
    "    observed = [3, 4, 4]\n",
    "    background = [i * interpolate(0.8, 1.0, 1.3, mu_b) for i in [3.0, 1.5, 1.0]]\n",
    "    signal = [i * mu for i in [0.7, 1.7, 1.5]]\n",
    "\n",
    "    print(f\"signal: {signal}\")\n",
    "    print(f\"background: {background}\")\n",
    "    print(f\"observed: {observed}\")\n",
    "    print(f\"likelihood scaled by: {normal_constraint(mu_b)/normal_constraint(0.0)}\\n\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(bins, background, 1.0, label=r\"Background\", edgecolor=\"red\", alpha=0.5)\n",
    "    ax.bar(\n",
    "        bins,\n",
    "        signal,\n",
    "        1.0,\n",
    "        label=r\"Signal\",\n",
    "        edgecolor=\"blue\",\n",
    "        bottom=background,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.scatter(bins, [3, 4, 4], color=\"black\", label=\"Observed\")\n",
    "    ax.set_ylim(0, 6)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Observable\", fontsize=12)\n",
    "    ax.set_ylabel(\"Count\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, notice that all along, we've been only discussing a single \"channel\" with 3 bins. The statistical analysis being studied might involve **multiple channels** corresponding to different analysis signal regions and control regions. Therefore, we compute the likelihood as\n",
    "\n",
    "$$\n",
    "p_\\text{main} = p_\\text{channel1} * p_\\text{channel2} * p_\\text{channel3} \\cdots\n",
    "$$\n",
    "\n",
    "We then expand out the likelihood definition further across channels\n",
    "\n",
    "$$\n",
    "p(n,a|\\theta) = \\underbrace{\\prod_{\\mathrm{channel}\\ c}\\prod_{\\mathrm{bin}\\ b} \\mathrm{Pois}(n_{cb} | \\lambda_{cb}(\\theta))}_{\\text{main}}\\, \\underbrace{\\prod_{\\mathrm{constraint}\\ \\chi} p_\\chi(a_\\chi | \\chi)}_{\\text{auxiliary}} \\qquad \\lambda_{cb}(\\theta) = \\sum_{\\mathrm{sample}\\ s} \\lambda_{cbs}(\\theta)\n",
    "$$\n",
    "\n",
    "There are now two pieces of the model:\n",
    "* the main model, which consists of\n",
    "  * several channels (regions, histograms, etc), where\n",
    "  * each channel is a set of Poissons measuring the bin count for an expected value, where\n",
    "  * the expected value is the sum of various samples, where\n",
    "  * each samples expected value can be a function of parameters (or modifiers)\n",
    "* the constraint model, which consists of\n",
    "  * constraint terms on model parameters, where\n",
    "  * each constraint term describes auxiliary measurements\n",
    "  \n",
    "It should be clear by now that this is quite a lot of pieces to keep track of. This is where HistFactory comes in to play. Using HistFactory, we can\n",
    "* describe observed event rates and expected event rates\n",
    "* use well-defined **modifiers** to express parameterizations of the expected event rates\n",
    "* use well-defined **interpolation** mechanisms to derive expected event rates (if needed)\n",
    "* automatically handle auxiliary measurements / additional constraint terms\n",
    "\n",
    "*Note: if you're curious about interpolation and interesting challenges, see the next chapter.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyhf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up till 2018, HistFactory was only implemented using ROOT, RooStats, RooFit (+ minuit). pyhf provides two separate pieces:\n",
    "* a schema for serializing the HistFactory workspace in plain-text formats, such as JSON\n",
    "* a toolkit that interacts and manipulates the HistFactory workspaces\n",
    "\n",
    "Why is this crucial? HistFactory in ROOT is a combination of loosely-linked XML+ROOT files\n",
    "* XML for structure\n",
    "* ROOT for storing data\n",
    "\n",
    "These would then be processed through a `hist2workspace` command to get the ROOT Workspace that RooStats/RooFit use. These XML/ROOT workspaces are (slowly) being deprecated in favor of [HS3](https://pypi.org/project/hs3/) (HEP Statistics Serialization Standard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to HistFactory Models\n",
    "\n",
    "ðŸŽ¶ I'm the very Model of a simple HEP-like measurement... ðŸŽ¶\n",
    "\n",
    "## HEP-like?\n",
    "\n",
    "So what do we do as experimentalists in High Energy Physics? We have a gigantic detector that serves as a counting machine for physics. We smash particles and take pictures of the aftermath. Then we propose hypotheses for new physics that we want to test using this data. So we will have, at the very least:\n",
    "\n",
    "* signal\n",
    "* background (with some uncertainty)\n",
    "* some observations\n",
    "\n",
    "This is probably the simplest model one could use in HistFactory... so let's make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pyhf.simplemodels.uncorrelated_background(\n",
    "    signal=[5.0, 10.0], bkg=[50.0, 60.0], bkg_uncertainty=[5.0, 12.0]\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we just make? This returns a [`pyhf.pdf.Model`](https://pyhf.readthedocs.io/en/v0.7.5/_generated/pyhf.pdf.Model.html#pyhf.pdf.Model) object. Let's check out the specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(model.spec, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the specification, we defined a single two-bin channel ('singlechannel') with two samples: a **signal sample** and a **background sample**. The signal sample has an _unconstrained normalization factor_ $\\mu$, the signal strength, while the background sample carries an _uncorrelated shape systematic_ controlled by parameters $\\gamma_{1}$ and $\\gamma_{2}$. The _background uncertainty_ for the bins is $10\\%$ and $20\\%$ respectively.\n",
    "\n",
    "These uncertainties are **absolute** (not relative!). We have 10% and 20% relative uncertainty for the bins in the background sample respectively.\n",
    "\n",
    "(*Note: we have a workspace defined with this simple model in `data/2-bin_1-channel.json` which we will look at later.*)\n",
    "\n",
    "Let's explore the model a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  channels: {model.config.channels}\")\n",
    "print(f\"     nbins: {model.config.channel_nbins}\")\n",
    "print(f\"   samples: {model.config.samples}\")\n",
    "print(f\" modifiers: {model.config.modifiers}\")\n",
    "print(f\"parameters: {model.config.parameters}\")\n",
    "print(f\"  nauxdata: {model.config.nauxdata}\")\n",
    "print(f\"   auxdata: {model.config.auxdata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Data\n",
    "\n",
    "Whoa, hold up! What's with the auxiliary data? Recall the HistFactory definition\n",
    "\n",
    "$$\n",
    "p(n,a|\\theta) = \\underbrace{\\prod_{\\mathrm{channel}\\ c}\\prod_{\\mathrm{bin}\\ b} \\mathrm{Pois}(n_{cb} | \\lambda_{cb}(\\theta))}_{\\text{main}} \\underbrace{\\prod_{\\mathrm{constraint}\\ \\chi} p_\\chi(a_\\chi | \\chi)}_{\\text{auxiliary}}\n",
    "$$\n",
    "\n",
    "for\n",
    "\n",
    "$$\n",
    "\\lambda_{cb}(\\theta) = \\sum_{\\mathrm{sample}\\ s} \\lambda_{cbs}(\\theta)\n",
    "$$\n",
    "\n",
    "and the auxiliary data here is passed into a constraint function. There's two things going on here:\n",
    "\n",
    "1. The uncorrelated shapesys modifier is constrained by a Poisson.\n",
    "2. The auxiliary data is fully determined by the shapesys 'data' and the 'background' data.\n",
    "\n",
    "So if we were to explicitly write out the likelihood we were seeing symbolically, it would be:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_\\text{main} \\cdot p_\\text{aux} &= \\text{Pois}(n | \\lambda) \\cdot p_\\text{aux}\\\\\n",
    "                                 &= \\text{Pois}(n | \\mu s + \\gamma b) \\cdot p_\\text{aux}\\\\\n",
    "                                 &= \\text{Pois}(n | \\mu s + \\gamma b) \\cdot \\text{Pois}(n_\\text{aux} = (b/\\delta b)^2 | \\mu = (b/\\delta b)^2 \\gamma )\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $n = \\{n_1, n_2\\}$ for a 2-bin model (we're being slightly fast and loose with our mathematical notation here), and similarly for $s$, $b$, and $\\gamma$.\n",
    "\n",
    "The 'shapesys' is defined in the [HistFactory paper](https://cds.cern.ch/record/1456844)... however, it can be a little hard to extract out the necessary information. We've provided a nice table of [Modifiers and Constraints](https://pyhf.readthedocs.io/en/v0.7.5/intro.html#id24) in the introduction of our pyhf documentation to use as reference.\n",
    "\n",
    "![modifiers and constraints](https://pyhf.github.io/pyhf-tutorial/_images/modifiers_and_constraints.png)\n",
    "\n",
    "Let's look at the first row here which is our uncorrelated shape modifier. This is a multiplicative modifier denoted by $\\kappa$ per-bin (denoted by $\\gamma_b$). Notice that the input for the constraint term requires $\\sigma_b$ which is the relative uncertainty of that modifier. This is Poisson-constrained by $\\sigma_b^{-2}$. Let's quickly calculate \"by hand\" the auxiliary data to convince ourselves of what's going on here (remembering that the background uncertainties were 10% and 20% of the observed background counts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array([5.0, 12.0]) / np.array([50.0, 60.0])) ** -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is what we see from the `pyhf.pdf.Model` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.auxdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Parameters\n",
    "\n",
    "What about the actual data for the model then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.expected_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hrmm, that didn't work. It seems we need to specify the parameter values to get the data for the model at those parameter values. Well, we know the default for $\\mu=1$ and $\\gamma = 1$. Also recall that the uncorrelated shape allocates a parameter per bin, so our model has **3 parameters** with **2 modifiers**. So let's just try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.expected_data([1.0, 1.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the expected data given the model parameters for the entire likelihood for the 2 bin model, the main model as well as the constraint (or auxiliary) model. We can also drop the auxdata to get the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.expected_data([1.0, 1.0, 1.0], include_auxdata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, there are also methods separately for the actual data and the auxdata, both which take in all the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.expected_actualdata([1.0, 1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.expected_auxdata([1.0, 1.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Parameter Ordering\n",
    "\n",
    "So how do we know what these parameters correspond to? The way `pyhf` works under the hood, you can imagine an `n`-dimensional tensor that is purely bookkeeping all the different expected rates and modifications (read: systematics) involved that we can apply tensor algebra calculations to. This means that the ordering of the entries in each dimension is important, so we need to keep track of the order of the channels, the samples, the parameters. In `pyhf`, you can ask for the parameters or modifiers in a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.modifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, these might **not necessarily** correspond with the order of the parameters when we build this tensor internally, and we keep this order under a different variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.par_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Parameter Multiplicity\n",
    "\n",
    "But this clearly doesn't answer everything, because we were using three numbers assigned to two parameters! How does that work? Well, recall that the only channel in this model has 2 bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.channel_nbins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and one of the parameters (the uncorrelated shape systematic) will be configured by $\\gamma_b$ &mdash; that is, one parameter for each bin of the sample in that channel. How does `pyhf` know that the parameter set is associated with two parameters? We can ask for information about the `paramset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.param_set(\"uncorr_bkguncrt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we can see is constrained by a Poisson and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.param_set(\"uncorr_bkguncrt\").n_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Parameter Defaults\n",
    "\n",
    "So how do we know what we should set the parameters to? The nice thing is that this is up to you! The model will come with some suggested defaults for parameters, their bounds, and whether it should be held constant in a fit or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_fixed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So armed with this knowledge, we could have also done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_pars = model.config.suggested_init()\n",
    "model.expected_actualdata(init_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the expected actual data from the model (signal + background), we could turn off the signal, corresponding with the normalization factor. We just need to know the index of the parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.poi_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then just change the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_pars = init_pars.copy()\n",
    "bkg_pars[model.config.poi_index] = 0\n",
    "model.expected_actualdata(bkg_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Inference\n",
    "\n",
    "The core of statistical analysis is the statistical model. For inference, it's viewed as a function of the model parameters conditioned on the fixed observations.\n",
    "\n",
    "$$\n",
    "\\log L(\\theta | x) \\propto \\log p(x | \\theta)\n",
    "$$\n",
    "\n",
    "The value of the likelihood is a float. Let's try it for both the background-only model as well as the signal+background model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [53.0, 65.0] + model.config.auxdata  # this is a common pattern!\n",
    "\n",
    "model.logpdf(pars=bkg_pars, data=observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.logpdf(pars=init_pars, data=observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not performing inference just yet. We're simply computing the 'logpdf' of the model specified by the parameters $\\theta$ against the provided data. To perform a fit, we use the [inference API](https://pyhf.readthedocs.io/en/v0.7.5/api.html#inference) via `pyhf.infer`.\n",
    "\n",
    "When fitting a model to data, we usually want to find the $\\hat{\\theta}$ which refers to the \"Maximum Likelihood Estimate\" of the model parameters. This is often referred to mathematically by\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_\\text{MLE} = \\text{argmax}_\\theta L(\\theta | x)\n",
    "$$\n",
    "\n",
    "Let's perform a unconstrained maximum likelihood fit on this model to the provided observations we just made up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhf.infer.mle.fit(data=observations, pdf=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what can we say? With nominal signal `[5, 10]` and nominal background = `[50, 60]` model components, an observed count of `[53, 65]` suggests best fit values:\n",
    "* $\\hat{\\mu} \\approx 0.54$,\n",
    "* $\\hat{\\gamma} \\approx [1,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Modifiers\n",
    "\n",
    "Let's go through an example to understand how modifiers work by using a slightly less simplified example with more bins and more samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "spec = {'measurements': [{'name': 'meas',\n",
    "   'config': {'poi': 'SigXsecOverSM',\n",
    "    'parameters': [{'name': 'lumi',\n",
    "      'auxdata': [1.0],\n",
    "      'bounds': [[0.5, 1.5]],\n",
    "      'inits': [1.0],\n",
    "      'sigmas': [0.1]},\n",
    "     {'name': 'SigXsecOverSM',\n",
    "      'bounds': [[0.0, 3.0]],\n",
    "      'inits': [1.0],\n",
    "      'fixed': False}]}}],\n",
    " 'channels': [{'name': 'channel1',\n",
    "   'samples': [{'name': 'signal',\n",
    "     'data': [0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0024999999441206455,\n",
    "      0.034999996423721313,\n",
    "      0.10749996453523636,\n",
    "      0.37999972701072693,\n",
    "      1.1575027704238892,\n",
    "      2.8950424194335938,\n",
    "      6.25261926651001,\n",
    "      12.101970672607422,\n",
    "      18.871929168701172,\n",
    "      26.27851104736328,\n",
    "      31.457117080688477,\n",
    "      31.42711067199707,\n",
    "      26.513561248779297,\n",
    "      19.279516220092773,\n",
    "      11.862010955810547,\n",
    "      6.697629451751709,\n",
    "      2.8975424766540527,\n",
    "      1.2050038576126099,\n",
    "      0.452499657869339,\n",
    "      0.10249996930360794,\n",
    "      0.019999997690320015,\n",
    "      0.004999999888241291,\n",
    "      0.0024999999441206455,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0],\n",
    "     'modifiers': [{'name': 'lumi', 'type': 'lumi', 'data': None},\n",
    "      {'name': 'SigXsecOverSM', 'type': 'normfactor', 'data': None}]},\n",
    "    {'name': 'qcd',\n",
    "     'data': [13.5,\n",
    "      15.9375,\n",
    "      16.875,\n",
    "      21.0,\n",
    "      20.25,\n",
    "      26.8125,\n",
    "      25.6875,\n",
    "      37.3125,\n",
    "      35.625,\n",
    "      41.0625,\n",
    "      43.3125,\n",
    "      50.0625,\n",
    "      48.0,\n",
    "      52.875,\n",
    "      62.8125,\n",
    "      70.125,\n",
    "      77.625,\n",
    "      78.75,\n",
    "      91.875,\n",
    "      92.4375,\n",
    "      106.5,\n",
    "      111.5625,\n",
    "      123.1875,\n",
    "      116.25,\n",
    "      135.0,\n",
    "      143.4375,\n",
    "      149.4375,\n",
    "      157.5,\n",
    "      170.4375,\n",
    "      185.25,\n",
    "      183.1875,\n",
    "      190.6875,\n",
    "      200.0625,\n",
    "      215.0625,\n",
    "      219.0,\n",
    "      233.0625,\n",
    "      232.3125,\n",
    "      240.0,\n",
    "      261.5625,\n",
    "      253.5,\n",
    "      260.625,\n",
    "      273.9375,\n",
    "      288.75,\n",
    "      278.4375,\n",
    "      299.4375,\n",
    "      303.375,\n",
    "      295.125,\n",
    "      303.375,\n",
    "      307.875,\n",
    "      295.6875],\n",
    "     'modifiers': [{'name': 'lumi', 'type': 'lumi', 'data': None}]},\n",
    "    {'name': 'mc1',\n",
    "     'data': [70.89239501953125,\n",
    "      85.42305755615234,\n",
    "      98.69751739501953,\n",
    "      107.05967712402344,\n",
    "      123.5215072631836,\n",
    "      134.2285919189453,\n",
    "      142.21739196777344,\n",
    "      153.22543334960938,\n",
    "      158.00746154785156,\n",
    "      156.2821807861328,\n",
    "      158.3825225830078,\n",
    "      156.60098266601562,\n",
    "      149.39981079101562,\n",
    "      141.1109619140625,\n",
    "      131.86570739746094,\n",
    "      122.71528625488281,\n",
    "      107.17217254638672,\n",
    "      95.02266693115234,\n",
    "      85.57305145263672,\n",
    "      73.21730041503906,\n",
    "      59.96159362792969,\n",
    "      49.16203308105469,\n",
    "      39.46867752075195,\n",
    "      31.481475830078125,\n",
    "      24.712656021118164,\n",
    "      18.768844604492188,\n",
    "      13.89379596710205,\n",
    "      10.650012969970703,\n",
    "      8.006237030029297,\n",
    "      5.681241989135742,\n",
    "      4.218745231628418,\n",
    "      3.0187482833862305,\n",
    "      1.8187508583068848,\n",
    "      1.3125004768371582,\n",
    "      0.7312501072883606,\n",
    "      0.41249996423721313,\n",
    "      0.35624998807907104,\n",
    "      0.29999998211860657,\n",
    "      0.11250000447034836,\n",
    "      0.07500000298023224,\n",
    "      0.11250000447034836,\n",
    "      0.0,\n",
    "      0.01875000074505806,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0],\n",
    "     'modifiers': [{'name': 'lumi', 'type': 'lumi', 'data': None},\n",
    "      {'name': 'mc1_weight_var1',\n",
    "       'type': 'histosys',\n",
    "       'data': {'lo_data': [67.34945678710938,\n",
    "         81.15284729003906,\n",
    "         93.76290893554688,\n",
    "         101.70653533935547,\n",
    "         117.34443664550781,\n",
    "         127.51444244384766,\n",
    "         135.10333251953125,\n",
    "         145.56053161621094,\n",
    "         150.103271484375,\n",
    "         148.46432495117188,\n",
    "         150.45956420898438,\n",
    "         148.7671661376953,\n",
    "         141.9263458251953,\n",
    "         134.05227661132812,\n",
    "         125.27024841308594,\n",
    "         116.57856750488281,\n",
    "         101.81340026855469,\n",
    "         90.27198791503906,\n",
    "         81.29533386230469,\n",
    "         69.55799865722656,\n",
    "         56.96500015258789,\n",
    "         46.70486831665039,\n",
    "         37.495689392089844,\n",
    "         29.9074649810791,\n",
    "         23.477069854736328,\n",
    "         17.830434799194336,\n",
    "         13.199125289916992,\n",
    "         10.117523193359375,\n",
    "         7.605934143066406,\n",
    "         5.397185325622559,\n",
    "         4.007810592651367,\n",
    "         2.867811441421509,\n",
    "         1.7278119325637817,\n",
    "         1.2468746900558472,\n",
    "         0.6946874856948853,\n",
    "         0.3918750286102295,\n",
    "         0.3384374976158142,\n",
    "         0.2849999964237213,\n",
    "         0.10687500238418579,\n",
    "         0.07124999910593033,\n",
    "         0.10687500238418579,\n",
    "         0.0,\n",
    "         0.017812499776482582,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0],\n",
    "        'hi_data': [74.43893432617188,\n",
    "         89.69686126708984,\n",
    "         103.6357192993164,\n",
    "         112.41641235351562,\n",
    "         129.70184326171875,\n",
    "         140.94131469726562,\n",
    "         149.32662963867188,\n",
    "         160.88104248046875,\n",
    "         165.90042114257812,\n",
    "         164.08950805664062,\n",
    "         166.29409790039062,\n",
    "         164.42413330078125,\n",
    "         156.86553955078125,\n",
    "         148.165283203125,\n",
    "         138.46115112304688,\n",
    "         128.85543823242188,\n",
    "         112.53453826904297,\n",
    "         99.77693939208984,\n",
    "         89.85436248779297,\n",
    "         76.88020324707031,\n",
    "         62.96103286743164,\n",
    "         51.62094497680664,\n",
    "         41.44242858886719,\n",
    "         33.05548858642578,\n",
    "         25.9482479095459,\n",
    "         19.70726203918457,\n",
    "         14.588473320007324,\n",
    "         11.182509422302246,\n",
    "         8.406550407409668,\n",
    "         5.965306282043457,\n",
    "         4.4296875,\n",
    "         3.1696882247924805,\n",
    "         1.9096863269805908,\n",
    "         1.3781245946884155,\n",
    "         0.7678126096725464,\n",
    "         0.43312501907348633,\n",
    "         0.37406253814697266,\n",
    "         0.3150000274181366,\n",
    "         0.11812499910593033,\n",
    "         0.07874999940395355,\n",
    "         0.11812499910593033,\n",
    "         0.0,\n",
    "         0.019687499850988388,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0]}},\n",
    "      {'name': 'mc1_shape_conv',\n",
    "       'type': 'histosys',\n",
    "       'data': {'lo_data': [101.05992126464844,\n",
    "         119.52792358398438,\n",
    "         124.30897521972656,\n",
    "         135.48504638671875,\n",
    "         146.54934692382812,\n",
    "         151.25636291503906,\n",
    "         156.7322540283203,\n",
    "         155.5133056640625,\n",
    "         156.52597045898438,\n",
    "         148.96849060058594,\n",
    "         144.9928436279297,\n",
    "         136.0101318359375,\n",
    "         123.95274353027344,\n",
    "         114.31562805175781,\n",
    "         102.69110107421875,\n",
    "         89.3228988647461,\n",
    "         78.4483413696289,\n",
    "         67.7237777709961,\n",
    "         55.16178894042969,\n",
    "         45.037200927734375,\n",
    "         37.893741607666016,\n",
    "         28.856449127197266,\n",
    "         22.70638656616211,\n",
    "         17.606334686279297,\n",
    "         13.312540054321289,\n",
    "         10.200008392333984,\n",
    "         7.01248836517334,\n",
    "         5.493741989135742,\n",
    "         3.59999680519104,\n",
    "         3.0374982357025146,\n",
    "         1.931250810623169,\n",
    "         1.031250238418579,\n",
    "         0.6750000715255737,\n",
    "         0.35624998807907104,\n",
    "         0.3749999701976776,\n",
    "         0.3749999701976776,\n",
    "         0.15000000596046448,\n",
    "         0.09375,\n",
    "         0.07500000298023224,\n",
    "         0.03750000149011612,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0],\n",
    "        'hi_data': [44.58721923828125,\n",
    "         55.19928741455078,\n",
    "         66.03634643554688,\n",
    "         79.6482925415039,\n",
    "         89.77288055419922,\n",
    "         104.9222640991211,\n",
    "         115.74057006835938,\n",
    "         126.57763671875,\n",
    "         135.82260131835938,\n",
    "         145.70545959472656,\n",
    "         150.93756103515625,\n",
    "         157.38861083984375,\n",
    "         155.4945526123047,\n",
    "         152.73785400390625,\n",
    "         151.7439422607422,\n",
    "         142.6112060546875,\n",
    "         136.55397033691406,\n",
    "         126.40888977050781,\n",
    "         113.11567687988281,\n",
    "         102.07238006591797,\n",
    "         91.81654357910156,\n",
    "         81.12948608398438,\n",
    "         63.092716217041016,\n",
    "         56.51173400878906,\n",
    "         44.137237548828125,\n",
    "         38.36247253417969,\n",
    "         29.400203704833984,\n",
    "         22.425132751464844,\n",
    "         17.306331634521484,\n",
    "         12.75003433227539,\n",
    "         9.675003051757812,\n",
    "         7.274987697601318,\n",
    "         5.587491989135742,\n",
    "         4.181245803833008,\n",
    "         2.887498617172241,\n",
    "         2.0062506198883057,\n",
    "         0.7312501072883606,\n",
    "         0.8062501549720764,\n",
    "         0.5249999761581421,\n",
    "         0.29999998211860657,\n",
    "         0.22499999403953552,\n",
    "         0.15000000596046448,\n",
    "         0.05625000223517418,\n",
    "         0.03750000149011612,\n",
    "         0.07500000298023224,\n",
    "         0.01875000074505806,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0]}}]},\n",
    "    {'name': 'mc2',\n",
    "     'data': [0.0,\n",
    "      0.03750000149011612,\n",
    "      0.15000000596046448,\n",
    "      0.15000000596046448,\n",
    "      0.4999999701976776,\n",
    "      0.9999996423721313,\n",
    "      2.024998903274536,\n",
    "      3.9250059127807617,\n",
    "      6.937517166137695,\n",
    "      11.612465858459473,\n",
    "      19.499845504760742,\n",
    "      30.124683380126953,\n",
    "      45.03794860839844,\n",
    "      61.551456451416016,\n",
    "      82.9527587890625,\n",
    "      106.87921905517578,\n",
    "      130.5798797607422,\n",
    "      153.2743377685547,\n",
    "      167.1709442138672,\n",
    "      177.41844177246094,\n",
    "      174.7440948486328,\n",
    "      165.29640197753906,\n",
    "      152.6619873046875,\n",
    "      129.00526428222656,\n",
    "      108.60432434082031,\n",
    "      82.77774810791016,\n",
    "      63.5015754699707,\n",
    "      46.113014221191406,\n",
    "      30.124683380126953,\n",
    "      18.787357330322266,\n",
    "      12.812447547912598,\n",
    "      7.300018787384033,\n",
    "      3.412503957748413,\n",
    "      1.9624994993209839,\n",
    "      1.0124995708465576,\n",
    "      0.5,\n",
    "      0.2625000476837158,\n",
    "      0.1875000149011612,\n",
    "      0.0625,\n",
    "      0.02500000037252903,\n",
    "      0.012500000186264515,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0,\n",
    "      0.0],\n",
    "     'modifiers': [{'name': 'lumi', 'type': 'lumi', 'data': None},\n",
    "      {'name': 'mc2_weight_var1',\n",
    "       'type': 'histosys',\n",
    "       'data': {'lo_data': [0.0,\n",
    "         0.035624999552965164,\n",
    "         0.14249999821186066,\n",
    "         0.14249999821186066,\n",
    "         0.4750000834465027,\n",
    "         0.9500001668930054,\n",
    "         1.923748254776001,\n",
    "         3.728752851486206,\n",
    "         6.590609550476074,\n",
    "         11.031888961791992,\n",
    "         18.525110244750977,\n",
    "         28.618989944458008,\n",
    "         42.78604507446289,\n",
    "         58.473121643066406,\n",
    "         78.80338287353516,\n",
    "         101.53242492675781,\n",
    "         124.04771423339844,\n",
    "         145.60733032226562,\n",
    "         158.80825805664062,\n",
    "         168.54275512695312,\n",
    "         166.00228881835938,\n",
    "         157.02755737304688,\n",
    "         145.025634765625,\n",
    "         122.55144500732422,\n",
    "         103.17119598388672,\n",
    "         78.63713073730469,\n",
    "         60.325645446777344,\n",
    "         43.807308197021484,\n",
    "         28.618989944458008,\n",
    "         17.84822654724121,\n",
    "         12.171903610229492,\n",
    "         6.9349822998046875,\n",
    "         3.2418766021728516,\n",
    "         1.8643739223480225,\n",
    "         0.9618750810623169,\n",
    "         0.4750000834465027,\n",
    "         0.24937501549720764,\n",
    "         0.17812499403953552,\n",
    "         0.05937499925494194,\n",
    "         0.023749999701976776,\n",
    "         0.011874999850988388,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0],\n",
    "        'hi_data': [0.0,\n",
    "         0.039374999701976776,\n",
    "         0.1574999988079071,\n",
    "         0.1574999988079071,\n",
    "         0.5250000357627869,\n",
    "         1.0500000715255737,\n",
    "         2.1262497901916504,\n",
    "         4.121241092681885,\n",
    "         7.2843523025512695,\n",
    "         12.19308090209961,\n",
    "         20.475082397460938,\n",
    "         31.631689071655273,\n",
    "         47.289207458496094,\n",
    "         64.62674713134766,\n",
    "         87.09906768798828,\n",
    "         112.22294616699219,\n",
    "         137.10789489746094,\n",
    "         160.93846130371094,\n",
    "         175.53074645996094,\n",
    "         186.29124450683594,\n",
    "         183.48301696777344,\n",
    "         173.56236267089844,\n",
    "         160.2954559326172,\n",
    "         135.45445251464844,\n",
    "         114.03438568115234,\n",
    "         86.91529846191406,\n",
    "         66.67443084716797,\n",
    "         48.41790771484375,\n",
    "         31.63167381286621,\n",
    "         19.726932525634766,\n",
    "         13.453075408935547,\n",
    "         7.664976119995117,\n",
    "         3.583118438720703,\n",
    "         2.060624122619629,\n",
    "         1.0631251335144043,\n",
    "         0.5250000357627869,\n",
    "         0.2756250202655792,\n",
    "         0.19687500596046448,\n",
    "         0.06562499701976776,\n",
    "         0.026249999180436134,\n",
    "         0.013124999590218067,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0]}},\n",
    "      {'name': 'mc2_shape_conv',\n",
    "       'type': 'histosys',\n",
    "       'data': {'lo_data': [0.2500000298023224,\n",
    "         0.550000011920929,\n",
    "         1.1499994993209839,\n",
    "         2.3374998569488525,\n",
    "         3.90000581741333,\n",
    "         6.7250165939331055,\n",
    "         11.649965286254883,\n",
    "         18.574859619140625,\n",
    "         26.9122314453125,\n",
    "         40.87519454956055,\n",
    "         56.60115432739258,\n",
    "         75.85232543945312,\n",
    "         94.82848358154297,\n",
    "         116.87982940673828,\n",
    "         137.49069213867188,\n",
    "         155.72373962402344,\n",
    "         164.5465850830078,\n",
    "         166.00872802734375,\n",
    "         165.9712371826172,\n",
    "         153.36181640625,\n",
    "         136.11602783203125,\n",
    "         116.96733093261719,\n",
    "         97.91616821289062,\n",
    "         77.68994140625,\n",
    "         55.63859558105469,\n",
    "         42.13777160644531,\n",
    "         27.962215423583984,\n",
    "         18.662357330322266,\n",
    "         11.462468147277832,\n",
    "         6.575016021728516,\n",
    "         4.237506866455078,\n",
    "         2.299999713897705,\n",
    "         0.8874996900558472,\n",
    "         0.5874999761581421,\n",
    "         0.27500003576278687,\n",
    "         0.13750000298023224,\n",
    "         0.0625,\n",
    "         0.012500000186264515,\n",
    "         0.0,\n",
    "         0.012500000186264515,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0],\n",
    "        'hi_data': [0.0,\n",
    "         0.0,\n",
    "         0.02500000037252903,\n",
    "         0.05000000074505806,\n",
    "         0.13750001788139343,\n",
    "         0.23750002682209015,\n",
    "         0.5375000238418579,\n",
    "         1.3374993801116943,\n",
    "         1.9749993085861206,\n",
    "         3.90000581741333,\n",
    "         6.6625165939331055,\n",
    "         11.262471199035645,\n",
    "         19.18735122680664,\n",
    "         27.074729919433594,\n",
    "         39.425106048583984,\n",
    "         57.513710021972656,\n",
    "         74.82726287841797,\n",
    "         95.94105529785156,\n",
    "         116.41729736328125,\n",
    "         138.10304260253906,\n",
    "         156.9734344482422,\n",
    "         166.0712127685547,\n",
    "         167.08346557617188,\n",
    "         162.69703674316406,\n",
    "         153.93667602539062,\n",
    "         138.16552734375,\n",
    "         119.36747741699219,\n",
    "         96.35357666015625,\n",
    "         72.9771499633789,\n",
    "         57.688720703125,\n",
    "         40.53767395019531,\n",
    "         28.412208557128906,\n",
    "         18.54986000061035,\n",
    "         11.399969100952148,\n",
    "         6.775016784667969,\n",
    "         3.7375051975250244,\n",
    "         2.299999713897705,\n",
    "         1.0749995708465576,\n",
    "         0.7124998569488525,\n",
    "         0.23750002682209015,\n",
    "         0.15000000596046448,\n",
    "         0.15000000596046448,\n",
    "         0.03750000149011612,\n",
    "         0.0,\n",
    "         0.012500000186264515,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0,\n",
    "         0.0]}}]}]}],\n",
    " 'observations': [{'name': 'channel1',\n",
    "   'data': [60.0,\n",
    "    103.0,\n",
    "    101.0,\n",
    "    116.0,\n",
    "    158.0,\n",
    "    156.0,\n",
    "    160.0,\n",
    "    172.0,\n",
    "    192.0,\n",
    "    214.0,\n",
    "    207.0,\n",
    "    252.0,\n",
    "    249.0,\n",
    "    279.0,\n",
    "    279.0,\n",
    "    302.0,\n",
    "    331.0,\n",
    "    335.0,\n",
    "    343.0,\n",
    "    335.0,\n",
    "    323.0,\n",
    "    351.0,\n",
    "    305.0,\n",
    "    303.0,\n",
    "    282.0,\n",
    "    242.0,\n",
    "    239.0,\n",
    "    217.0,\n",
    "    208.0,\n",
    "    197.0,\n",
    "    214.0,\n",
    "    202.0,\n",
    "    224.0,\n",
    "    199.0,\n",
    "    224.0,\n",
    "    218.0,\n",
    "    261.0,\n",
    "    239.0,\n",
    "    264.0,\n",
    "    254.0,\n",
    "    275.0,\n",
    "    268.0,\n",
    "    262.0,\n",
    "    300.0,\n",
    "    258.0,\n",
    "    284.0,\n",
    "    301.0,\n",
    "    289.0,\n",
    "    309.0,\n",
    "    283.0]}],\n",
    " 'version': '1.0.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = pyhf.Workspace(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the spec, we construct a probability density function (p.d.f). As the model includes systematics, it will be a simultaneous joint p.d.f. of the main model (poisson) and constraint model. The latter is defined by the implied \"auxiliary measurements\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = workspace.model(measurement_name='meas')\n",
    "data = workspace.data(pdf)\n",
    "# what is the measurement?\n",
    "workspace.get_measurement(measurement_name='meas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p.d.f is build from one data-drived \"qcd\" (or multijet) estimate and two Monte Carlo-based background samples and is parametrized by five parameters: One parameter of interest `SigXsecOverSM` and four *nuisance parameters* that affect the shape of the two Monte Carlo background estimates (both weight-only and shape systematics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Samples:\\n {pdf.config.samples}')\n",
    "print(f'Parameters:\\n {pdf.config.parameters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Exploration of a HistFactory Model\n",
    "\n",
    "One advantage of a pure-python implementation of Histfactory is the ability to explore the pdf interactively within the setting of a notebook. Try moving the sliders and oberserve the effect on the samples. For example changing the parameter of interest `SigXsecOverSM` (or `Âµ`) controls the overall normalization of the (BSM) signal sample (`Âµ=0` for background-only and `Âµ=1` for the nominal signal-plus-background hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "par_name_dict = {k: v[\"slice\"].start for k, v in pdf.config.par_map.items()}\n",
    "all_par_settings = {\n",
    "    n[0]: tuple(m)\n",
    "    for n, m in zip(\n",
    "        sorted(reversed(list(par_name_dict.items())), key=lambda x: x[1]),\n",
    "        (*pdf.config.suggested_bounds(),0.1),\n",
    "    )\n",
    "}\n",
    "\n",
    "def get_mc_counts(pars):\n",
    "    deltas, factors = pdf._modifications(pars)\n",
    "    allsum = pyhf.tensorlib.concatenate(\n",
    "        deltas + [pyhf.tensorlib.astensor(pdf.nominal_rates)]\n",
    "    )\n",
    "    nom_plus_delta = pyhf.tensorlib.sum(allsum, axis=0)\n",
    "    nom_plus_delta = pyhf.tensorlib.reshape(\n",
    "        nom_plus_delta, (1,) + pyhf.tensorlib.shape(nom_plus_delta)\n",
    "    )\n",
    "    allfac = pyhf.tensorlib.concatenate(factors + [nom_plus_delta])\n",
    "    return pyhf.tensorlib.product(allfac, axis=0)\n",
    "\n",
    "animate_plot_pieces = None\n",
    "\n",
    "\n",
    "def init_plot(fig, ax):\n",
    "    global animate_plot_pieces\n",
    "\n",
    "    nbins = sum(list(pdf.config.channel_nbins.values()))\n",
    "    x = np.arange(nbins)\n",
    "    data = np.zeros(nbins)\n",
    "    items = []\n",
    "    for i in [3, 2, 1, 0]:\n",
    "        items.append(ax.bar(x, data, 1, alpha=1.0))\n",
    "    animate_plot_pieces = (\n",
    "        items,\n",
    "        ax.scatter(\n",
    "            x, workspace.data(pdf, include_auxdata=False), c=\"k\", alpha=1.0, zorder=99\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def animate(ax=None, fig=None, **par_settings):\n",
    "    global animate_plot_pieces\n",
    "    items, obs = animate_plot_pieces\n",
    "    pars = pyhf.tensorlib.astensor(pdf.config.suggested_init())\n",
    "    for k, v in par_settings.items():\n",
    "        pars[par_name_dict[k]] = v\n",
    "\n",
    "    mc_counts = get_mc_counts(pars)\n",
    "    rectangle_collection = zip(*map(lambda x: x.patches, items))\n",
    "\n",
    "    for rectangles, binvalues in zip(rectangle_collection, mc_counts[:, 0].T):\n",
    "        offset = 0\n",
    "        for sample_index in [3, 2, 1, 0]:\n",
    "            rect = rectangles[sample_index]\n",
    "            binvalue = binvalues[sample_index]\n",
    "            rect.set_y(offset)\n",
    "            rect.set_height(binvalue)\n",
    "            offset += rect.get_height()\n",
    "\n",
    "    fig.canvas.draw()\n",
    "\n",
    "def plot(ax=None, order=[3, 2, 1, 0], **par_settings):\n",
    "    pars = pyhf.tensorlib.astensor(pdf.config.suggested_init())\n",
    "    for k, v in par_settings.items():\n",
    "        pars[par_name_dict[k]] = v\n",
    "\n",
    "    mc_counts = get_mc_counts(pars)\n",
    "    bottom = None\n",
    "    # nb: bar_data[0] because evaluating only one parset\n",
    "    for i, sample_index in enumerate(order):\n",
    "        data = mc_counts[sample_index][0]\n",
    "        x = np.arange(len(data))\n",
    "        ax.bar(x, data, 1, bottom=bottom, alpha=1.0)\n",
    "        bottom = data if i == 0 else bottom + data\n",
    "    ax.scatter(\n",
    "        x, workspace.data(pdf, include_auxdata=False), c=\"k\", alpha=1.0, zorder=99\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(10, 5)\n",
    "ax.set_ylim(0, 1.5 * np.max(workspace.data(pdf, include_auxdata=False)))\n",
    "\n",
    "init_plot(fig, ax)\n",
    "interact(animate, fig=fixed(fig), ax=fixed(ax), **all_par_settings);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with this in hand, we can go ahead and check how things look for background, signal+background, and the best fit parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal = pdf.config.suggested_init()\n",
    "background_only = pdf.config.suggested_init()\n",
    "background_only[pdf.config.poi_index] = 0.0\n",
    "best_fit = pyhf.infer.mle.fit(data, pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, sharex=True)\n",
    "fig.set_size_inches(9, 3)\n",
    "ax1.set_ylim(0, 1.5 * np.max(workspace.data(pdf, include_auxdata=False)))\n",
    "ax1.set_title('sig + bkg Âµ = 1')\n",
    "plot(ax=ax1, **{k: nominal[v] for k, v in par_name_dict.items()})\n",
    "\n",
    "ax2.set_title('bkg-only Âµ = 0')\n",
    "plot(ax=ax2, **{k: background_only[v] for k, v in par_name_dict.items()})\n",
    "\n",
    "ax3.set_title(f'best fit Âµ = {best_fit[pdf.config.poi_index]:.3g}')\n",
    "plot(ax=ax3, **{k: best_fit[v] for k, v in par_name_dict.items()});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using HEPData\n",
    "\n",
    "As of this tutorial, ATLAS has [published 33 full statistical models to HEPData](https://pyhf.github.io/public-probability-models/atlas.json)\n",
    "\n",
    "<p align=\"center\">\n",
    "<a href=\"https://www.hepdata.net/record/ins1755298?version=3\"><img src=\"https://raw.githubusercontent.com/matthewfeickert/talk-SciPy-2020/e0c509cd0dfef98f5876071edd4c60aff9199a1b/figures/HEPData_likelihoods.png\"></a>\n",
    "</p>\n",
    "\n",
    "Let's explore the 1Lbb workspace a little bit shall we?\n",
    "\n",
    "### Getting the Data\n",
    "\n",
    "We'll use the `pyhf[contrib]` extra (which relies on `requests` and `tarfile`) to download the HEPData minted DOI and extract the files we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhf.contrib.utils.download(\n",
    "    \"https://doi.org/10.17182/hepdata.90607.v3/r3\", \"1Lbb-likelihoods\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will nicely download and extract everything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lavh 1Lbb-likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate our objects\n",
    "\n",
    "We have a background-only workspace `BkgOnly.json` and a signal patchset collection `patchset.json`. Let's create our python objects and play with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = json.load(open(\"1Lbb-likelihoods/BkgOnly.json\"))\n",
    "patchset = pyhf.PatchSet(json.load(open(\"1Lbb-likelihoods/patchset.json\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what did the analyzers give us for signal patches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patching in Signals\n",
    "\n",
    "Let's look at this [`pyhf.PatchSet`](https://pyhf.readthedocs.io/en/v0.7.5/_generated/pyhf.patchset.PatchSet.html#pyhf.patchset.PatchSet) object which provides a user-friendly way to interact with many signal patches at once.\n",
    "\n",
    "#### PatchSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh wow, we've got 125 patches. What information does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"description: {patchset.description}\")\n",
    "print(f\"    digests: {patchset.digests}\")\n",
    "print(f\"     labels: {patchset.labels}\")\n",
    "print(f\" references: {patchset.references}\")\n",
    "print(f\"    version: {patchset.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've got a useful description of the signal patches... there's a digest. Does that match the background-only workspace we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhf.utils.digest(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does! In fact, this sort of verification check will be done automatically when applying patches using `pyhf.PatchSet` as we will see shortly. To manually verify, simply run `pyhf.PatchSet.verify` on the workspace. No error means everything is fine. It will loudly complain otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchset.verify(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No error, whew. Let's move on.\n",
    "\n",
    "The labels `m1` and `m2` tells us that we have the signal patches parametrized in 2-dimensional space, likely as $m_1 = \\tilde{\\chi}_1^\\pm$ and $m_2 = \\tilde{\\chi}_1^0$... but I guess we'll see?\n",
    "\n",
    "The references list the references for this dataset, which is pointing at the hepdata record for now.\n",
    "\n",
    "Next, the version is the version of the schema set we're using with `pyhf` (`1.0.0`).\n",
    "\n",
    "And last, but certainly not least... its patches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchset.patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see all the patches listed both by name such as `C1N2_Wh_hbb_900_250` as well as a pair of points `(900, 250)`. Why is this useful? The `PatchSet` object acts like a special dictionary look-up where it will grab the patch you need based on the unique key you provide it.\n",
    "\n",
    "For example, we can look up by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchset[\"C1N2_Wh_hbb_900_250\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or by the pair of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchset[(900, 250)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patches\n",
    "\n",
    "A `pyhf.PatchSet` is a collection of `pyhf.Patch` objects. What is a patch indeed? It contains enough information about how to apply the signal patch to the corresponding background-only workspace (matched by digest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = patchset[\"C1N2_Wh_hbb_900_250\"]\n",
    "print(f\"  name: {patch.name}\")\n",
    "print(f\"values: {patch.values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, it contains the patch information itself. Specifically, this inherits from the `jsonpatch.JsonPatch` object, which is a 3rd party module providing native support for json patching in python. That means we can simply apply the patch to our workspace directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" samples pre-patch: {pyhf.Workspace(spec).samples}\")\n",
    "print(f\"samples post-patch: {pyhf.Workspace(patch.apply(spec)).samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, more quickly, from the `PatchSet` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" samples pre-patch: {pyhf.Workspace(spec).samples}\")\n",
    "print(f\"samples post-patch: {pyhf.Workspace(patchset.apply(spec, (900, 250))).samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patching via Model Creation\n",
    "\n",
    "One last way to apply the patching is to, instead of patching workspaces, we patch the models as we build them from the background-only workspace. This maybe makes it easier to treat the background-only workspace as immutable, and patch in signal models when grabbing the model. Check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = pyhf.Workspace(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load up our background-only spec into the workspace. Then let's create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = workspace.model(patches=[patchset[\"C1N2_Wh_hbb_900_250\"]])\n",
    "print(f\"samples (workspace): {workspace.samples}\")\n",
    "print(f\"samples (  model  ): {model.config.samples}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
