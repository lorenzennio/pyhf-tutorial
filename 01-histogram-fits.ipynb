{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade pip\n",
    "# ! pip install --user pyhf numpy matplotlib json iminuit ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram fits with `pyhf`\n",
    "\n",
    "Often we don't have a clear way to parametrize our fit templates, so we need to resort to MC simulations and use histograms as templates that we fit to data in the same bins.\n",
    "\n",
    "We are going to use the [`pyhf`](https://github.com/scikit-hep/pyhf) package for these fits. The documentation can be found at https://pyhf.readthedocs.io/.\n",
    "\n",
    "This is an adapted version of the notebook presented [here](https://github.com/nikoladze/HEPFittingTutorial/blob/master/examples/jupyter_notebooks/006_histogram_fits_with_pyhf.ipynb). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create 2 artificial histograms with 10 bins (having 11 bin boundaries). You could imagine these as two different background processes for which we have MC simulations on which we ran some event selection and created histograms for. For now, let's assume that the shape of these distributions comes out correctly and we only need to fit the normalization (for both templates independently) to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(11)\n",
    "bin_cents = (bins[1:]+bins[:-1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1 = np.array([1.5, 3., 6., 7.5, 6.3, 6.6, 6., 4.5, 3. , 1.5])\n",
    "hist2 = np.array([3. , 6., 9., 12., 15., 9. , 6., 3. , 0.3, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stairs(hist1, bins)\n",
    "plt.stairs(hist2, bins)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to stack them since we think the sum of both should give us the expected data yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stairs(hist1, bins, fill=True)\n",
    "plt.stairs(hist1+hist2, bins, fill=True, baseline=hist1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's assume we observed the following data counts in each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([ 4, 17, 26, 23, 34, 23, 21,  7,  8,  4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the data to the plot, including Poisson errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stairs(hist1, bins, fill=True)\n",
    "plt.stairs(hist1+hist2, bins, fill=True, baseline=hist1)\n",
    "plt.errorbar(bin_cents, data, fmt='ok', yerr=np.sqrt(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One template fits it all\n",
    "\n",
    "`pyhf` does fits using the Maximum-Likelihood method and uses the HistFactory ([CERN-OPEN-2012-016](https://cds.cern.ch/record/1456844)) template. In the simplemost case the pdf (probability density function) is just a product of Poisson counts in each bin:\n",
    "\n",
    "$$L(\\vec n|\\vec\\lambda) = \\prod_{\\mathrm{bin}\\, b} \\mathrm{Pois}(n_b | \\lambda_b)$$\n",
    "\n",
    "where $\\mathrm{Pois}(n_b | \\lambda_b)$ is the Poisson distribution for $\\lambda_b$ expected and $n_b$ observed counts. In our case $\\lambda_b$ would be given by\n",
    "\n",
    "$$\\lambda_b = \\mu_1 b_{1b} + \\mu_2 b_{2b}$$\n",
    "\n",
    "where $b_{1b}$ and $b_{2b}$ are the expected counts in bin $b$ of our 2 histograms and $\\mu_1$ and $\\mu_2$ are the normalization factors we want to fit. This pdf will define the Likelihood function that is later maximized to give the best fitting parameter values.\n",
    "\n",
    "The general template is more complicated, allowing for constraint terms and separation into arbitrary channels - we will come back to that later.\n",
    "\n",
    "Models in `pyhf` are defined with a json-like specification. That is, a nested structure of lists and dictionaries. The hierarchy is as follows:\n",
    "\n",
    "* A model can have several **channels**. This can be used to e.g. separate signal and control regions\n",
    "* Each channel can have several **samples**. Each sample comes with a histogram template.\n",
    "* Each sample can have several **modifiers**. These will define the fit parameters. Modifiers can be free normalization factors or constraint parameters (more later)\n",
    "\n",
    "In our case we can define a model with just one channel and two samples which each have a normalization factor (free parameter) as a modifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    {\n",
    "        \"name\": \"sample1\",\n",
    "        \"data\": list(hist1),\n",
    "        \"modifiers\": [\n",
    "            {\"name\": \"mu1\", \"type\": \"normfactor\", \"data\" : None}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"sample2\",\n",
    "        \"data\": list(hist2),\n",
    "        \"modifiers\": [\n",
    "            {\"name\": \"mu2\", \"type\": \"normfactor\", \"data\" : None}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "spec = {\"channels\" : [{\"name\" : \"singlechannel\", \"samples\" : samples}]}\n",
    "\n",
    "# info: the `poi_name=None` is nescessary here since we don't want to do a hypothesis test\n",
    "model = pyhf.Model(spec, poi_name=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    The histogram bin contents need to be specified as a list (not a numpy array), such that we can really dump this into the text based json format.\n",
    "</div>\n",
    "\n",
    "Our specification would look like this as a json string (which can be simply stored in a text file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(model.spec, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run a *maximum likelihood fit* that gives us the parameters that maximize the likelihood (technically we will minimize the negative log-likelihood), the *maximum likelihood estimates* (mle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1, mu2 = pyhf.infer.mle.fit(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1, mu2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not have to specify initial parameter values or bounds. For normalization factors the initial parameters are by default `1` and the bounds (fit range) is `[0, 10]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_bounds()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the fitted templates, together with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stairs(mu1*hist1, bins, fill=True)\n",
    "plt.stairs(mu1*hist1+mu2*hist2, bins, fill=True, baseline=hist1)\n",
    "plt.errorbar(bin_cents, data, fmt='ok', yerr=np.sqrt(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainties on fit parameters and the \"post-fit\" plot\n",
    "\n",
    "Often, we are also interested in the uncertainties and correlations between fit parameters. We can use `iminuit` as a fitting backend for `pyhf` to extract them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhf.set_backend('numpy', 'minuit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, correlations = pyhf.infer.mle.fit(data, model, return_uncertainties=True, return_correlations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainties on the histogram templates\n",
    "\n",
    "Our histogram templates are usually derived by MC simulations. We typically want to assign uncertainties on the templates themselves. Those can be of different origin, but mostly fall into the following categories:\n",
    "\n",
    "* **MC stat. error**: Statistical uncertainty due to the limited simulated sample size. The relative uncertainty on the expected count in a histogram bin is given by $\\sqrt{N}/N$ for a bin with $N$ simulated events or\n",
    "$$\\frac{\\sqrt{\\sum_{\\text{events i in bin b}}w_i^2}}{{\\sum_{\\text{events i in bin b}} w_i}}$$ \n",
    "for weighted events with weights $w_i$ per event.\n",
    "* **Experimental uncertainties**: Uncertainties on detector simulation or reconstruction/calibration. Many of them are evaluated by re-running the full analysis chain with certain parameters varied up and down by one standard deviation of some measured/calibrated parameters.\n",
    "* **Theory uncertainties**: Uncertainties on cross sections and on the choice of different theoretical models/approximations (e.g. parton shower) or parameters. Cross section uncertainties affect the normalization, while the others are typically evaluated by re-running the simulation with parameters changed or models/algorithms replaced.\n",
    "\n",
    "In all cases we need to provide additional input to parametrize the Likelihood template. In `pyhf` this is done by specifying `modifiers` for the samples we want to assign uncertainties to. It is important to have an overview of these modifiers. Have a look at\n",
    "\n",
    "https://pyhf.readthedocs.io/en/v0.7.2/likelihood.html#modifiers\n",
    "\n",
    "before you move on.\n",
    "\n",
    "Each modifier will come with additional parameters in the Likelihood template. We will discuss the 2 main classes:\n",
    "\n",
    "* Uncorrelated uncertainties per bin\n",
    "    * Special case: MC statistical uncertainty\n",
    "* Correlated uncertainty across all bins\n",
    "\n",
    "By default, different modifiers are independent. On the other hand, correlated shape modifiers and normalization uncertainties have compatible constraint terms and thus modifiers can be declared that share parameters by re-using a name for multiple modifiers. That is, a variation of a single parameter causes a shift within sample rates due to both shape and normalization variations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncorrelated uncertainties per bin\n",
    "\n",
    "Uncorrelated in this sense means the uncertainty affects each bin individually.\n",
    "\n",
    "Let's assume we want to fit just one histogram this time (using `hist1` as a template) and we have determined the (absolute) uncertainty per bin as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1_uncorr_err = np.array([0.4 , 0.4 , 0.3 , 0.2 , 0.15, 0.4 , 0.45, 0.5 , 0.3 , 0.35])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to our normalization factor, we add another modifier to the sample of type `shapesys`. This will create a template with an additional uncertainty for each bin. We need to specify the uncertainty per bin for the `\"data\"` entry of the modifier dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    {\n",
    "        \"name\": \"sample1\",\n",
    "        \"data\": list(hist1),\n",
    "        \"modifiers\": [\n",
    "            # first modifier: normalization factor\n",
    "            {\"name\": \"mu1\", \"type\": \"normfactor\", \"data\" : None},\n",
    "            # second modifier: uncorrelated uncertainties per bin\n",
    "            {\n",
    "                \"name\": \"uncorrelated_uncertainties\",\n",
    "                \"type\": \"shapesys\",\n",
    "                \"data\" : list(hist1_uncorr_err)\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "spec = {\"channels\" : [{\"name\" : \"singlechannel\", \"samples\" : samples}]}\n",
    "\n",
    "model_uncorr = pyhf.Model(spec, poi_name=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will introduce one additional fit parameter per bin, so we have 11 parameters in total, 1 for the normalization factor and 10 for the uncorrelated uncertainty per bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uncorr.config.npars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pdf for building the likelihood function becomes\n",
    "\n",
    "$$L(\\vec n, \\vec a|\\vec \\lambda, \\vec \\gamma) = \\prod_{\\mathrm{data\\,bin}\\, b} \\mathrm{Pois}(n_b | \\lambda_b) \\prod_{\\mathrm{aux\\,data\\, bin}\\, b} \\mathrm{Pois}(a_b | \\gamma_b a_b)$$\n",
    "\n",
    "where in our example\n",
    "\n",
    "$$\\lambda_b = \\mu_b \\gamma_b b_b$$\n",
    "\n",
    "We build these uncertainties into our model by adding **auxiliary data** that emulates this. The most common interpretation of this auxiliary data is that the uncertain parameter $\\gamma_b a_b$ follows a Poisson distribution (prior). However, this way of thinking is manifestly bayesian. If the parameter was estimated from an auxiliary measurement, then it is the PDF for that measurement that we wish to include into our probability model. In the frequentist way of thinking, the underlying parameters that define our uncertainty have an unknown true value and upon repeating the experiment many times the auxiliary measurements estimating these parameters would fluctuate randomly about this true value.\n",
    "\n",
    "We determine the amount of auxiliary data $a_b$ per bin by asking \"which effective number of entries would give us that relative uncertainty\". That is given by\n",
    "\n",
    "$$\\frac{\\sqrt{a_b}}{a_b} = \\frac{\\delta_b}{b_b} \\rightarrow a_b = \\left(\\frac{b_b}{\\delta_b}\\right)^2$$\n",
    "\n",
    "where $b_b$ is the expected count and $\\delta_b$ the absolute uncertainty in the corresponding histogram bin.\n",
    "\n",
    "So the auxiliary data for each bin is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list((hist1 / hist1_uncorr_err) ** 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is also what `pyhf` calculated for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uncorr.config.auxdata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auxdata is set to match the expected counts exactly for the initial parameter values $\\gamma_b = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_parameters = model_uncorr.config.suggested_init()\n",
    "plt.stairs(model_uncorr.expected_auxdata(initial_parameters), bins)\n",
    "plt.errorbar(bin_cents,model_uncorr.config.auxdata, yerr=np.sqrt(model_uncorr.config.auxdata), fmt='o')\n",
    "print(initial_parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the parameters $\\gamma_b$ need to simultaneously fit the real and auxiliary data. They are therefore not completely free. We say they are \"constrained\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to manually tune the parameters in the following interactive plot. Observe how the templates that need to fit real and auxiliary data change. Also have a look at the value of the negative log likelihood (which is the objective we want to minimize in the fit). Why can't we get a perfect fit to the actual data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to understand right now what is happening in this code cell, first focus on the application\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "sliders_gamma = {\n",
    "    f\"gamma{i}\" : widgets.FloatSlider(\n",
    "        1.0,\n",
    "        min=0.1,\n",
    "        max=2.0,\n",
    "        orientation=\"vertical\",\n",
    "        continuous_update=False,\n",
    "        description=f\"γ{i}\",\n",
    "        layout=widgets.Layout(width='35px')\n",
    "    )\n",
    "    for i in range(1, 11)\n",
    "}\n",
    "\n",
    "slider_mu = widgets.FloatSlider(\n",
    "    1.0, min=0.1, max=10.0, description=\"Norm factor\", continuous_update=False\n",
    ")\n",
    "\n",
    "def plot(mu, **kwargs):\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(20, 5))\n",
    "\n",
    "    parameters = model_uncorr.config.suggested_init()\n",
    "    \n",
    "    parameters[0] = mu\n",
    "    \n",
    "    for k in kwargs:\n",
    "        i = int(k.replace(\"gamma\", \"\"))\n",
    "        parameters[i] = kwargs[k]\n",
    "\n",
    "    axs[0].stairs(model_uncorr.expected_actualdata(parameters), bins)\n",
    "    axs[0].errorbar(bin_cents, data, yerr=np.sqrt(data), fmt='ok')\n",
    "    axs[1].stairs(model_uncorr.expected_auxdata(parameters), bins)\n",
    "    param_set = model_uncorr.config.param_set(\"uncorrelated_uncertainties\")\n",
    "    axs[1].errorbar(\n",
    "        bin_cents,\n",
    "        model_uncorr.config.auxdata,\n",
    "        yerr=model_uncorr.config.auxdata * np.array(param_set.width()),\n",
    "        fmt='or'\n",
    "    )\n",
    "    axs[0].set_title(\"Actual data\")\n",
    "    axs[1].set_title(\"Auxiliary data\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    print(\n",
    "        \"Negative Log-Likelihood: \"\n",
    "        f\"{- model_uncorr.logpdf(parameters, np.concatenate([data, model_uncorr.config.auxdata]))[0]:.3f}\"\n",
    "    )\n",
    "    \n",
    "interactive_plot = widgets.interactive_output(plot, dict(sliders_gamma, mu=slider_mu))\n",
    "interactive_plot.layout.height = \"450px\"\n",
    "\n",
    "def fit(b):\n",
    "    parameters = pyhf.infer.mle.fit(np.concatenate([data, model_uncorr.config.auxdata]), model_uncorr)\n",
    "    slider_mu.value = parameters[0]\n",
    "    for k in sliders_gamma:\n",
    "        i = int(k.replace(\"gamma\", \"\"))\n",
    "        sliders_gamma[k].value = parameters[i]\n",
    "        \n",
    "button = widgets.Button(description=\"Fit\")\n",
    "button.on_click(fit)\n",
    "\n",
    "display(\n",
    "    slider_mu,\n",
    "    widgets.HBox(\n",
    "        [widgets.HBox(layout=widgets.Layout(width='45px'))]\n",
    "        + [sliders_gamma[f\"gamma{i}\"] for i in range(1, 11)]\n",
    "    ),\n",
    "    button,\n",
    "    interactive_plot\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special case: Statistical uncertainties\n",
    "\n",
    "As the sample counts are often derived from Monte Carlo (MC) datasets, they necessarily carry an uncertainty due to the finite sample size of the datasets. Adding uncertainties for each sample would yield a very large number of nuisance parameters with limited utility. Therefore, a set of bin-wise scale factors $\\gamma_{b}$ is introduced to model the overall uncertainty in the bin due to MC statistics.\n",
    "\n",
    "This effectively means that we can add statistical errors as `staterror` modifier, and they will be quadratically combined across samples for each bin, given that they carry the same modifier name. Channels are still kept separate.\n",
    "\n",
    "The constraint term for the `staterror` modifier is a product of Gaussians over bins ([which might change in the future](https://github.com/scikit-hep/pyhf/issues/760)). The pdf for building the likelihood function becomes\n",
    "\n",
    "$$L(\\vec n, \\vec a|\\vec \\lambda, \\vec \\gamma) = \\prod_{\\mathrm{data\\,bin}\\, b} \\mathrm{Pois}(n_b | \\lambda_b ) \\prod_{\\mathrm{aux\\,data\\, bin}\\, b} \\text{Gauss}(a_b=1 | \\gamma_b, \\sigma_b)$$\n",
    "\n",
    "where in our example\n",
    "\n",
    "$$\\lambda_b = \\gamma_b \\mu_{1b} b_{1b} + \\gamma_b \\mu_{2b} b_{2b},$$\n",
    "\n",
    "since we have two samples now (`sample1` and `sample2`). The $\\gamma_b$ are factors of around 1 and shared between samples and\n",
    "\n",
    "$$\\sigma_b = \\sqrt{\\sum_{\\mathrm{samples}\\, s} \\delta_{sb}^2} \\Bigg/ \\sum_{\\mathrm{samples}\\, s} b_{sb},$$\n",
    "\n",
    "where $b_{sb}$ are the bin counts and $\\delta_{sb}$ are the absolute uncertainties in each bin, for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    {\n",
    "        \"name\": \"sample1\",\n",
    "        \"data\": list(hist1),\n",
    "        \"modifiers\": [\n",
    "            {\"name\": \"mu1\", \"type\": \"normfactor\", \"data\" : None},\n",
    "            {\n",
    "                \"name\": \"statistical_error\",                        # same name\n",
    "                \"type\": \"staterror\",\n",
    "                \"data\" : list(np.sqrt(hist1))                       # absolute yield uncertainties in each bin (Poisson error)\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"sample2\",\n",
    "        \"data\": list(hist2),\n",
    "        \"modifiers\": [\n",
    "            {\"name\": \"mu2\", \"type\": \"normfactor\", \"data\" : None},\n",
    "            {\n",
    "                \"name\": \"statistical_error\",                        # same name\n",
    "                \"type\": \"staterror\",\n",
    "                \"data\" : list(np.sqrt(hist2))                       # absolute yield uncertainties in each bin (Poisson error)\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "spec = {\"channels\" : [{\"name\" : \"singlechannel\", \"samples\" : samples}]}\n",
    "\n",
    "model_stat = pyhf.Model(spec, poi_name=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect 2 modifiers for the normalizations and 10 modifiers due to the statistical uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stat.config.npars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what the model calculated for the $\\sigma_b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stat.config.param_set(\"statistical_error\").width()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify this by hand using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.sqrt(hist1 + hist2)/(hist1+hist2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to understand right now what is happening in this code cell, first focus on the application\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "sliders_gamma = {\n",
    "    f\"gamma{i}\" : widgets.FloatSlider(\n",
    "        1.0,\n",
    "        min=0.1,\n",
    "        max=2.0,\n",
    "        orientation=\"vertical\",\n",
    "        continuous_update=False,\n",
    "        description=f\"γ{i}\",\n",
    "        layout=widgets.Layout(width='35px')\n",
    "    )\n",
    "    for i in range(1, 11)\n",
    "}\n",
    "\n",
    "slider_mu1 = widgets.FloatSlider(\n",
    "    1.0, min=0.1, max=10.0, description=\"Norm fact. 1\", continuous_update=False\n",
    ")\n",
    "\n",
    "slider_mu2 = widgets.FloatSlider(\n",
    "    1.0, min=0.1, max=10.0, description=\"Norm fact. 2\", continuous_update=False\n",
    ")\n",
    "\n",
    "par_slice = model_stat.config.par_slice\n",
    "\n",
    "def plot(mu1, mu2, **kwargs):\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(20, 5))\n",
    "\n",
    "    parameters = model_stat.config.suggested_init()\n",
    "    \n",
    "    parameters[par_slice(\"mu1\").start] = mu1\n",
    "    parameters[par_slice(\"mu2\").start] = mu2\n",
    "    \n",
    "    for k in kwargs:\n",
    "        i = int(k.replace(\"gamma\", \"\"))+1\n",
    "        parameters[i] = kwargs[k]\n",
    "    \n",
    "    expected_data = model_stat.main_model.expected_data(parameters, return_by_sample=True)\n",
    "    axs[0].stairs(expected_data[0], bins, fill=True, label='sample1')\n",
    "    axs[0].stairs(expected_data[0] + expected_data[1], bins, fill=True, baseline=expected_data[0], label='sample2')\n",
    "    axs[0].errorbar(bin_cents, data, yerr=np.sqrt(data), fmt='ok')\n",
    "    axs[1].stairs(model_stat.expected_auxdata(parameters), bins)\n",
    "    param_set = model_stat.config.param_set(\"statistical_error\")\n",
    "    axs[1].errorbar(\n",
    "        bin_cents,\n",
    "        model_stat.config.auxdata,\n",
    "        yerr=model_stat.config.auxdata * np.array(param_set.width()),\n",
    "        fmt='or'\n",
    "    )\n",
    "    axs[0].set_title(\"Actual data\")\n",
    "    axs[0].legend()\n",
    "    axs[1].set_title(\"Auxiliary data\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    print(\n",
    "        \"Negative Log-Likelihood: \"\n",
    "        f\"{- model_stat.logpdf(parameters, np.concatenate([data, model_stat.config.auxdata]))[0]:.3f}\"\n",
    "    )\n",
    "    \n",
    "interactive_plot = widgets.interactive_output(plot, dict(sliders_gamma, mu1=slider_mu1, mu2=slider_mu2))\n",
    "interactive_plot.layout.height = \"450px\"\n",
    "\n",
    "def fit(b):\n",
    "    parameters = pyhf.infer.mle.fit(np.concatenate([data, model_stat.config.auxdata]), model_stat)\n",
    "    slider_mu1.value = parameters[par_slice(\"mu1\").start]\n",
    "    slider_mu2.value = parameters[par_slice(\"mu2\").start]\n",
    "    \n",
    "    for k in sliders_gamma:\n",
    "        i = int(k.replace(\"gamma\", \"\"))+1\n",
    "        sliders_gamma[k].value = parameters[i]\n",
    "        \n",
    "button = widgets.Button(description=\"Fit\")\n",
    "button.on_click(fit)\n",
    "\n",
    "display(\n",
    "    slider_mu1,\n",
    "    slider_mu2,\n",
    "    widgets.HBox(\n",
    "        [widgets.HBox(layout=widgets.Layout(width='45px'))]\n",
    "        + [sliders_gamma[f\"gamma{i}\"] for i in range(1, 11)]\n",
    "    ),\n",
    "    button,\n",
    "    interactive_plot\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated uncertainty on template shape\n",
    "\n",
    "Another important type of uncertainty is a parameter correlated across all bins that changes the shape of the template as a whole.\n",
    "\n",
    "This time the constraint term is a standard normal distribution and the auxiliary data is set to 0. To get the correct impact on our histogram templates, each bin is interpolated such that it ends up at the \"low\" value when the nuisance parameter $\\alpha=-1$, at the expected value for $\\alpha=0$ and at the \"high\" value for $\\alpha=1$. It is also extrapolated for values of $\\alpha$ outside that range.\n",
    "\n",
    "We won't discuss the details of the interpolation functions here, but if you are interested, here they are:\n",
    "\n",
    "https://pyhf.readthedocs.io/en/v0.7.2/api.html#interpolators\n",
    "\n",
    "The default for the `histosys` modifier currently is \"piecewise-linear interpolation strategy, with polynomial at $|a|<1$\", `code4p`.\n",
    "\n",
    "The pdf for building the likelihood function becomes\n",
    "\n",
    "$$L(\\vec n, \\vec a|\\vec \\lambda, \\vec \\gamma) = \\prod_{\\mathrm{data\\,bin}\\, b} \\mathrm{Pois}(n_b | \\lambda_b ) \\, \\text{Gauss}(a=0 | \\alpha, \\sigma=1)$$\n",
    "\n",
    "Typically this is used for systematic uncertainties where we recreate the histogram for MC created with some experimental parameter varied up and down by 1 standard deviation (typically done centrally by the experiment's performance groups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    {\n",
    "        \"name\": \"sample1\",\n",
    "        \"data\": list(hist1 + 0.5 * hist2),\n",
    "        \"modifiers\": [\n",
    "            # first modifier: normalization factor\n",
    "            {\"name\": \"mu1\", \"type\": \"normfactor\", \"data\" : None},\n",
    "            # second modifier: correlated uncertainty on template shape\n",
    "            {\n",
    "                \"name\": \"alpha\",\n",
    "                \"type\": \"histosys\",\n",
    "                \"data\": {\n",
    "                    \"lo_data\": list(hist1),\n",
    "                    \"hi_data\": list(hist1 + hist2)\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "spec = {\"channels\" : [{\"name\" : \"singlechannel\", \"samples\" : samples}]}\n",
    "\n",
    "model_corr =  pyhf.Model(spec, poi_name=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we only have 1 extra parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_corr.config.par_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slider_alpha = widgets.FloatSlider(\n",
    "    0, min=-2, max=2, description=\"Alpha\", continuous_update=False\n",
    ")\n",
    "\n",
    "slider_mu = widgets.FloatSlider(\n",
    "    1.0, min=0.1, max=10.0, description=\"Norm factor\", continuous_update=False\n",
    ")\n",
    "\n",
    "par_slice = model_corr.config.par_slice\n",
    "\n",
    "def plot(mu, alpha):\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(15, 4))\n",
    "\n",
    "    parameters = model_corr.config.suggested_init()\n",
    "    \n",
    "    parameters[par_slice(\"mu1\").start] = mu\n",
    "    parameters[par_slice(\"alpha\").start] = alpha\n",
    "\n",
    "    axs[0].stairs(model_corr.expected_actualdata(parameters), bins)\n",
    "    axs[0].errorbar(bin_cents, data, yerr=np.sqrt(data), fmt='ok')\n",
    "    axs[0].stairs(hist1, bins, fill=True, alpha=0.2, label='hist1')\n",
    "    axs[0].stairs(hist1+hist2, bins, fill=True, alpha=0.2, baseline=hist1, label='hist2')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    axs[1].stairs(model_corr.expected_auxdata(parameters), [0, 1])\n",
    "    param_set = model_corr.config.param_set(\"alpha\")\n",
    "    axs[1].errorbar([0.5], model_corr.config.auxdata, yerr=np.array(param_set.width()), fmt='or')\n",
    "    \n",
    "    axs[0].set_title(\"Actual data\")\n",
    "    axs[1].set_title(\"Auxiliary data\")\n",
    "    \n",
    "    print(\n",
    "        \"Negative Log-Likelihood: \"\n",
    "        f\"{- model_corr.logpdf(parameters, np.concatenate([data, model_corr.config.auxdata]))[0]:.3f}\"\n",
    "    )\n",
    "    \n",
    "interactive_plot = widgets.interactive_output(plot, dict(mu=slider_mu, alpha=slider_alpha))\n",
    "interactive_plot.layout.height = \"400px\"\n",
    "\n",
    "def fit(b):\n",
    "    parameters = pyhf.infer.mle.fit(np.concatenate([data, model_corr.config.auxdata]), model_corr)\n",
    "    slider_mu.value = parameters[par_slice(\"mu1\").start]\n",
    "    slider_alpha.value = parameters[par_slice(\"alpha\").start]\n",
    "        \n",
    "button = widgets.Button(description=\"Fit\")\n",
    "button.on_click(fit)\n",
    "\n",
    "display(\n",
    "    slider_mu,\n",
    "    slider_alpha,\n",
    "    button,\n",
    "    interactive_plot\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
